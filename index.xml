<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dr. Sebastian Castano</title>
    <link>https://jscastanoc.github.io/</link>
    <description>Recent content on Dr. Sebastian Castano</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 05 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://jscastanoc.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Foundation Models for Time Series—A Case Study Using Brain Signals</title>
      <link>https://jscastanoc.github.io/blog/foundation-models-for-time-series/</link>
      <pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://jscastanoc.github.io/blog/foundation-models-for-time-series/</guid>
      <description>&lt;p&gt;For this post, I wanted to take a look at general purpose time series foundation models. In particular, I am curious about what the “general” part of these models actually entails. It is not intended as a throughout benchmark, but more as an exploratory exercise.&lt;/p&gt;
&lt;p&gt;I will start with a short intro about why foundation models for time series are in a different category compared to language or vision models, with an overview on what has been released the last year or so.&lt;/p&gt;</description>
      <content>&lt;p&gt;For this post, I wanted to take a look at general purpose time series foundation models. In particular, I am curious about what the “general” part of these models actually entails. It is not intended as a throughout benchmark, but more as an exploratory exercise.&lt;/p&gt;
&lt;p&gt;I will start with a short intro about why foundation models for time series are in a different category compared to language or vision models, with an overview on what has been released the last year or so.&lt;/p&gt;
&lt;p&gt;Then, I will transition to a case study with one of these models—&lt;a href=&#34;https://github.com/moment-timeseries-foundation-model/moment-research&#34;&gt;&lt;strong&gt;MOMENT&lt;/strong&gt;&lt;/a&gt;—by analyzing &lt;a href=&#34;https://en.wikipedia.org/wiki/Electroencephalography&#34;&gt;Electroencephalographic&lt;/a&gt; (EEG) data from a brain-computer interface experiment. Why EEG data? well, earlier this year, &lt;a href=&#34;https://piramidal.ai/&#34;&gt;Piramidal&lt;/a&gt;—a YC-backed company building a foundational model &lt;em&gt;specifically&lt;/em&gt; for EEG data—raised a $6.5M seed round. That left me wondering how good general-purpose models can be if companies are raising rounds this size for training domain-specific models.&lt;/p&gt;
&lt;p&gt;Impatient? &lt;a href=&#34;./blog/foundation-models-for-time-series/#tldr&#34;&gt;TLDR;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;why-are-general-time-series-foundation-models-different-from-other-modalities&#34;&gt;Why are general time series foundation models different from other modalities?&lt;/h2&gt;
&lt;p&gt;Language and vision have fundamental structures that are largely consistent, regardless of the domain. With language, for example, models trained on a math textbook and Shakespearean prose will still capture structural rules—syntax, grammar, sentence formation—. Domain-specific vocabulary or context can vary, but the foundational structure of language doesn’t change. Similarly, vision models can learn shapes, edges, colors, spatial patterns and textures that carry semantic meaning.&lt;/p&gt;
&lt;p&gt;However, when it comes to time series, I initially had some questions about the potential for such broad generalizability. Time series data holds information far beyond trends and seasonality: complex dynamics like phase and frequency modulation, as well as phase-amplitude coupling, can govern many processes and can drastically vary from one domain to the other. And this doesn’t even account for multi-channel data, where cross-channel interactions add yet another layer of complexity.&lt;/p&gt;
&lt;h2 id=&#34;overview-of-time-series-foundation-models&#34;&gt;Overview of Time Series Foundation Models&lt;/h2&gt;
&lt;p&gt;Here’s an overview of some pre-trained models for time series, along with the companies or labs behind them:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Model Link&lt;/th&gt;
          &lt;th&gt;Company / University&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/google-research/timesfm&#34;&gt;&lt;strong&gt;TimesFM&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Google&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/time-series-foundation-models/lag-llama&#34;&gt;&lt;strong&gt;Lag-Llama&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/abacusai/ForecastPFN?tab=readme-ov-file#inference-with-pretrained-model-&#34;&gt;&lt;strong&gt;ForecastPFN&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Abacus.ai&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/ibm-granite/granite-tsfm/tree/main/tsfm_public/models/tinytimemixer&#34;&gt;&lt;strong&gt;TinyTimeMixer&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;IBM&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/SalesforceAIResearch/uni2ts&#34;&gt;&lt;strong&gt;Uni2TS&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Salesforce&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/moment-timeseries-foundation-model/moment-research&#34;&gt;&lt;strong&gt;Moment&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;CMU&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://blog.salesforceairesearch.com/moirai/&#34;&gt;&lt;strong&gt;MOIRAI&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Salesforce&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://www.datadoghq.com/blog/datadog-time-series-foundation-model/&#34;&gt;&lt;strong&gt;Toto&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Datadog&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/amazon-research/amazon-chronos&#34;&gt;&lt;strong&gt;Chronos&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Amazon&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://docs.nixtla.io/&#34;&gt;TimeGPT&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;NIXTLA&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Many of these models are pre-trained on data from diverse domains—weather, medicine (e.g., ECGs), finance, and more. The pre-training approach for most of these models is similar to that used in vision models: masks are applied at various time points, and the model learns to predict the values for these masked intervals, effectively filling in the gaps.&lt;/p&gt;
&lt;p&gt;Ideally, pre-training enables these models to identify and understand informative patterns inherent across domains. Beyond basic seasonality and trends, a robust foundation model should capture higher-level structures such as phase and frequency modulations and cross-channel interactions. This level of adaptability would make such models truly general-purpose, equipping them to handle the intricate dynamics present in complex time series data. However, given the vast range of domains and the diversity of signal types, achieving this level of adaptability seems very challenging. The differences in underlying structures across domains makes it difficult for any single model to effectively capture the full complexity of &lt;em&gt;any type of&lt;/em&gt; time series data, which would explain the emergence domain-specific foundation models such as the one the developed by Piramidal.&lt;/p&gt;
&lt;h2 id=&#34;case-study&#34;&gt;Case study&lt;/h2&gt;
&lt;p&gt;As exploration exercise , I chose &lt;a href=&#34;https://github.com/moment-timeseries-foundation-model/moment-research&#34;&gt;&lt;strong&gt;MOMENT&lt;/strong&gt;&lt;/a&gt;. Reported results look very good. Besides that, it is very well documented. For the data, I chose an EEG dataset corresponding to a standard classification task for a brain-computer interface. More on the data below.&lt;/p&gt;
&lt;p&gt;The goal is to assess the zero-shot performance of the MOMENT embeddings, as used in an LDA classifier downstream. The code below can also be found &lt;a href=&#34;https://github.com/jscastanoc/castano-blog-notebooks/blob/main/fmodel-ts-classification/moment-bci.ipynb&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-set-up-general-parameters&#34;&gt;Step: Set up general parameters&lt;/h3&gt;
&lt;p&gt;We&amp;rsquo;ll be using the small version of the model and a subset of the EEG channels to simplify the analysis.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;uv pip install numpy pandas scikit&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;learn matplotlib tqdm torch moabb pyriemann
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;uv pip install git&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;https:&lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt;github&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;com&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;moment&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;timeseries&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;foundation&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;model&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;moment&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;git
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;TORCH_DEVICE &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mps&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# Apple Silicon GPU&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;MODEL_NAME &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AutonLab/MOMENT-1-small&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;EEG_CHANNELS &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Fz&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;C3&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Cz&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;C4&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;P3&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Pz&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;P4&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;O1&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;O2&amp;#39;&lt;/span&gt;] 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;SAMPLING_FREQUENCY &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;512&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-load-the-data&#34;&gt;Step: Load the data&lt;/h3&gt;
&lt;p&gt;We will use one of the sessions of the ERP study published by &lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0175856&#34;&gt;Hübner et al. 2017&lt;/a&gt; and available via de Mother Of All BCI Benchmarks &lt;a href=&#34;http://moabb.neurotechx.com/docs/index.html&#34;&gt;MOABB&lt;/a&gt;. The experiment in a nutshell: The patients are presented a rapid sequence of different visual stimuli, some of which they are instructed to pay attention to—target stimuli—and some of which they have to ignore—non-target stimuli—. It is expected that the brain response to each class is different.&lt;/p&gt;
&lt;p&gt;The dataset contains data for 13 subjects participating in 3 sessions each. It is not the scope of this post to do a throughout benchmark, so we will limit the dataset to one of the sessions of one of the subjects.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; contextlib
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; io
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; warnings
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.preprocessing &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; LabelEncoder
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; moabb &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; datasets
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; moabb.paradigms &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; P300
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; datasets&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Huebner2017(interval&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;.99&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;download()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;paradigm &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; P300(    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    resample&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;SAMPLING_FREQUENCY,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    baseline&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    channels&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;EEG_CHANNELS
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;stdout &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; io&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;StringIO()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; contextlib&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;redirect_stdout(stdout), warnings&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;catch_warnings(record&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; w:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    X, y, metadata &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; paradigm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_data(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        dataset&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;dataset,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        subjects&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        return_epochs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        return_raws&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        cache_config&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        postprocess_pipeline&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Limit to one session&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;session &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ids_mask_session &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; metadata&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;session &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; session
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y_encoded &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; LabelEncoder()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_transform(y)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; X[ids_mask_session]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y_encoded[ids_mask_session]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print number of classes and number of samples per class&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;unique_classes, counts &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unique(y, return_counts&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Number of classes: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;len(unique_classes)&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; cls, count &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; zip(unique_classes, counts):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Class &amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;cls&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;count&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; samples&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Number of classes: 2
Class &#39;0&#39;: 3275 samples
Class &#39;1&#39;: 1008 samples
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step-explore-the-data&#34;&gt;Step: Explore the data&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s take a look at what the average brain activity is for each target and non target stimuli, for each of the channels. I have highlighted two periods, where we find archetypical responses for this type of stimuli, the so-called N100 and P300: a (N)egative spike around 100ms after stimuli, and a (P)ositive spike around 300ms after stimuli. N100 is to be seen predominantly in electrodes around the visual cortex, i.e., Occital Channels O1 and O2, where as P300 is commonly seen in Central-Parietal, closer to the top of the skull, i.e., Cz and Pz.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; plotly.express &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; px
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; plotly.graph_objects &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; go
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; plotly.subplots &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; make_subplots
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ids_target &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;X_target &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; X[ids_target]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ids_nontarget &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;X_nontarget &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; X[ids_nontarget]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;X_target_avg &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; X_target&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;X_nontarget_avg &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; X_nontarget&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;num_channels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; X_target_avg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create a figure with subplots in a square grid&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;num_cols &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ceil(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt(num_channels)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;num_rows &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ceil(num_channels &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; num_cols))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;fig &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; make_subplots(rows&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;num_rows, cols&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;num_cols, subplot_titles&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;EEG_CHANNELS, shared_xaxes&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;, shared_yaxes&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;, vertical_spacing&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.05&lt;/span&gt;, horizontal_spacing&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.02&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;time_vector: np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ndarray &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(X_target_avg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;SAMPLING_FREQUENCY
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Plot each channel in a separate subplot&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(num_channels):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    row: int &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; num_cols &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    col: int &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; num_cols &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    showlegend: bool &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (i &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    fig&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_trace(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        go&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Scatter(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;time_vector, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;X_target_avg[i], 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            mode&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;lines&amp;#39;&lt;/span&gt;, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Target&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; showlegend &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            line&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;dict(color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;red&amp;#39;&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            showlegend&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;showlegend
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        ), 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        row&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;row, col&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;col
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    fig&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_trace(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        go&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Scatter(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;time_vector, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;X_nontarget_avg[i], 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            mode&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;lines&amp;#39;&lt;/span&gt;, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;NonTarget&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; showlegend &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            line&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;dict(color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            showlegend&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;showlegend
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        ), 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        row&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;row, col&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;col
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    fig&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_vrect(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x0&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;60&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;SAMPLING_FREQUENCY, x1&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;SAMPLING_FREQUENCY, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        fillcolor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;#EFCB66&amp;#34;&lt;/span&gt;, opacity&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        layer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;below&amp;#34;&lt;/span&gt;, line_width&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        row&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;row, col&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;col
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    fig&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_vrect(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x0&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;SAMPLING_FREQUENCY, x1&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;256&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;SAMPLING_FREQUENCY, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        fillcolor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;#90EE90&amp;#34;&lt;/span&gt;, opacity&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        layer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;below&amp;#34;&lt;/span&gt;, line_width&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        row&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;row, col&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;col
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; num_rows:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        fig&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;update_xaxes(title_text&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;seconds after stimulus&amp;#34;&lt;/span&gt;, row&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;row, col&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;col)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; col &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        fig&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;update_yaxes(title_text&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;µV&amp;#34;&lt;/span&gt;, row&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;row, col&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;col)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Update layout&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y_axis_range: list[float] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [min(X_nontarget_avg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;min(), X_target_avg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;min()), max(X_nontarget_avg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max(), X_target_avg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max())]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;fig&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;update_layout(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    height&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;800&lt;/span&gt;, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    width&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    showlegend&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, num_rows &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    fig&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;update_yaxes(range&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;y_axis_range, row&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;row, col&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;fig&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-baseline-pipeline&#34;&gt;Step: Baseline pipeline&lt;/h3&gt;
&lt;p&gt;As baseline, we will implemented a pipeline that is commonly used with this type of data. It consist of a &lt;em&gt;spatial filter&lt;/em&gt;, which means, a linear mix across all channels. The spatial filters are learned from data, such that the difference between classes is maximized. This is the &lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/4760273&#34;&gt;XDawn&lt;/a&gt; part of the pipeline.&lt;/p&gt;
&lt;p&gt;The spatial filters are followed by a standard LDA classifier.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; get_scorer
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pyriemann.estimation &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Xdawn
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.discriminant_analysis &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; LinearDiscriminantAnalysis &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; LDA
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.pipeline &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; make_pipeline
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.model_selection &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; train_test_split
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; mne.decoding &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Vectorizer
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;X_train, X_test, y_train, y_test &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_test_split(X, y, test_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.3&lt;/span&gt;, random_state&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;42&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;baseline_pipeline &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; make_pipeline(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Xdawn(nfilter&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Vectorizer(),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    LDA(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        solver&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;lsqr&amp;#39;&lt;/span&gt;, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        shrinkage&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;auto&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Fit the baseline pipeline&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;baseline_pipeline&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train, y_train)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Predict on the test set&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y_pred &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; baseline_pipeline&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the test score&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;test_score: float &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get_scorer(paradigm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scoring)(baseline_pipeline, X_test, y_test)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Test ROC-AUC XDAWN+LDA: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;test_score&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Test ROC-AUC XDAWN+LDA: 0.9788115284974094
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;pas mal.&lt;/p&gt;
&lt;h3 id=&#34;step-data-preparation-for-the-moment-torch-mdoel&#34;&gt;Step: Data preparation for the MOMENT torch mdoel&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;N_INPUT_SAMPLES &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;512&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.preprocessing &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; LabelEncoder
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; torch.utils.data &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; TensorDataset, DataLoader
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.model_selection &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; train_test_split
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;prepare_data&lt;/span&gt;(X_train, X_test, y_train, y_test):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Convert to torch tensors&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    X_train_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tensor(X_train, dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    X_test_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tensor(X_test, dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y_train_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tensor(y_train, dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;long)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y_test_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tensor(y_test, dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;long)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# pad the input to 512&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    X_train_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;functional&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pad(X_train_tensor, (&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, N_INPUT_SAMPLES &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; X_train_tensor&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    X_test_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;functional&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pad(X_test_tensor, (&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, N_INPUT_SAMPLES &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; X_test_tensor&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    input_mask: torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cat([torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ones(X_train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bool), torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros(N_INPUT_SAMPLES &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; X_train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bool)])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    input_mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; input_mask&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unsqueeze(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;repeat(X_train_tensor&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Create TensorDataset&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    train_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; TensorDataset(X_train_tensor, y_train_tensor, input_mask[:X_train_tensor&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    test_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; TensorDataset(X_test_tensor, y_test_tensor, input_mask[:X_test_tensor&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Create DataLoader&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    train_dataloader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataLoader(train_dataset, batch_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;, shuffle&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    test_dataloader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataLoader(test_dataset, batch_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;, shuffle&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; train_dataloader, test_dataloader
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;train_dataloader, test_dataloader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; prepare_data(X_train, X_test, y_train, y_test)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Just making sure that the conversion of data to torch tensors went OK&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Get tensors from dataloader&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;collect_tensors&lt;/span&gt;(dataloader: DataLoader) &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; tuple[torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Tensor, torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Tensor, torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Tensor]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    X_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    mask_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; X, y, mask &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; dataloader:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        X_list&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(X)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y_list&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(y) 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        mask_list&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(mask)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cat(X_list), torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cat(y_list), torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cat(mask_list)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;X_train_tensor, y_train_tensor, train_input_mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; collect_tensors(train_dataloader)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;X_test_tensor, y_test_tensor, test_input_mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; collect_tensors(test_dataloader)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Apply baseline_pipeline to X_train_tensor and X_test_tensor&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;baseline_pipeline&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train_tensor&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cpu()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;numpy(), y_train_tensor&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cpu()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;numpy())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Predict on the test set&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y_pred_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; baseline_pipeline&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test_tensor&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cpu()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;numpy())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the test score&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;test_score_tensor: float &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get_scorer(paradigm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scoring)(baseline_pipeline, X_test_tensor&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cpu()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;numpy(), y_test_tensor&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cpu()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;numpy())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Test ROC-AUC XDAWN+LDA: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;test_score_tensor&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Test ROC-AUC XDAWN+LDA: 0.9788147668393783
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;calculate-moment-embeddings&#34;&gt;Calculate MOMENT embeddings&lt;/h3&gt;
&lt;p&gt;We calculate the embeddings for each channel individually. It’s important to note that this setup is inherently biased against MOMENT, as it isn’t trained to learn across channels. To address this, we reshape the data so that each channel’s embedding is obtained separately, and then we concatenate these embeddings. Consequently, the baseline pipeline extracts inter-channel information, while MOMENT does not.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; tqdm &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tqdm
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; momentfm &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; MOMENTPipeline
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_embedding&lt;/span&gt;(model, dataloader):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    embeddings, labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [], []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;no_grad():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; batch_x, batch_labels, batch_mask &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; tqdm(dataloader, total&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;len(dataloader)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            batch_size, n_channels, n_timesteps &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch_x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            batch_x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch_x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(batch_size &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; n_channels, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, n_timesteps)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to(TORCH_DEVICE)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            batch_mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch_mask&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to(TORCH_DEVICE)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model(x_enc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;batch_x) &lt;span style=&#34;color:#75715e&#34;&gt;# [batch_size * n_channels x emb_dim]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            embedding &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; output&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;embeddings&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cpu()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(batch_size, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# [batch_size x n_channels * emb_dim]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            embeddings&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(embedding)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            labels&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(batch_labels)        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    embeddings, labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concatenate(embeddings), np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;concatenate(labels)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; embeddings, labels
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; MOMENTPipeline&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_pretrained(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    MODEL_NAME, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    model_kwargs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;task_name&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;embedding&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;init()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to(TORCH_DEVICE)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;train_embeddings, train_labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get_embedding(model, train_dataloader)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;test_embeddings, test_labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get_embedding(model, test_dataloader)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(train_embeddings&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape, train_labels&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(test_embeddings&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape, test_labels&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;(2998, 4608) (2998,)
(1285, 4608) (1285,)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step-downstream-classification&#34;&gt;Step: Downstream classification&lt;/h3&gt;
&lt;p&gt;We use an LDA and hyperparameter optimized SVM&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; momentfm.models.statistical_classifiers &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; fit_svm
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;moment_lda_pipeline &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; make_pipeline(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    LDA(solver&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;lsqr&amp;#39;&lt;/span&gt;, shrinkage&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;auto&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;moment_lda_pipeline&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(train_embeddings, train_labels)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y_pred &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; moment_lda_pipeline&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(test_embeddings)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;test_score &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get_scorer(paradigm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scoring)(moment_lda_pipeline, test_embeddings, test_labels)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Test ROC-AUC MOMENT+LDA: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;test_score&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Test ROC-AUC MOMENT+LDA: 0.609520725388601
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;moment_svm_pipeline &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; fit_svm(train_embeddings, train_labels)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y_pred &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; moment_svm_pipeline&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(test_embeddings)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;test_score &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get_scorer(paradigm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scoring)(moment_svm_pipeline, test_embeddings, test_labels)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Test ROC-AUC MOMENT+SVM: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;test_score&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Test ROC-AUC MOMENT+SVM: 0.549556347150259
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well, that is not very good. I suspect that it has to do with the dimensionality of the input data (number of channels x the number of embedding dimensions). Let me try once again but this time applying PCA before the LDA classifier to reduce the number of input dimensions by 90%.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.decomposition &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; PCA
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;moment_lda_pipeline &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; make_pipeline(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    PCA(n_components&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;int(train_embeddings&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;)),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    LDA(solver&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;lsqr&amp;#39;&lt;/span&gt;, shrinkage&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;auto&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;moment_lda_pipeline&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(train_embeddings, train_labels)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y_pred &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; moment_lda_pipeline&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(test_embeddings)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;test_score &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get_scorer(paradigm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scoring)(moment_lda_pipeline, test_embeddings, test_labels)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Test ROC-AUC MOMENT+PCA+LDA: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;test_score&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Test ROC-AUC MOMENT+PCA+LDA: 0.6235427461139896
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It improved, although not as much as I would have expected.&lt;/p&gt;
&lt;h3 id=&#34;step-dimensionality-reduction-before-embeddings&#34;&gt;Step: Dimensionality reduction before embeddings&lt;/h3&gt;
&lt;p&gt;I want to try one last thing. Since MOMENT is still not built to merge information across channels, we will help it a bit by spatially filtering the data &lt;em&gt;before&lt;/em&gt; calculating the embeddings. This should substantially increase the classification score.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x_dawn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Xdawn(nfilter&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;X_train_filtered &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x_dawn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_transform(X_train, y_train)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;X_test_filtered &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x_dawn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transform(X_test)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;train_dataloader_filtered, test_dataloader_filtered &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; prepare_data(X_train_filtered, X_test_filtered, y_train, y_test)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;train_embeddings_filtered, train_labels_filtered &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get_embedding(model, train_dataloader_filtered)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;test_embeddings_filtered, test_labels_filtered &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get_embedding(model, test_dataloader_filtered)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;moment_lda_pipeline &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; make_pipeline(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    LDA(solver&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;lsqr&amp;#39;&lt;/span&gt;, shrinkage&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;auto&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;moment_lda_pipeline&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(train_embeddings_filtered, train_labels_filtered)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y_pred &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; moment_lda_pipeline&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(test_embeddings_filtered)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;test_score &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get_scorer(paradigm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scoring)(moment_lda_pipeline, test_embeddings_filtered, test_labels_filtered)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Test ROC-AUC Xdawn+MOMENT+LDA: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;test_score&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Test ROC-AUC Xdawn+MOMENT+LDA: 0.8285330310880828
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Aha! the assumption of inter-channel independence in MOMENT appears to be a significant challenge. While we’re still not reaching the baseline’s 0.97 ROC-AUC, we’ve seen a clear improvement from 0.64 to 0.81 by spatially mixing the channels and, thus, reducing the channel count before calculating embeddings.&lt;/p&gt;
&lt;h4 id=&#34;remaining-considerations-for-this-exercise&#34;&gt;Remaining considerations for this exercise:&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Cross-subject, cross-session: I still wonder who well these embeddings work if we merge the data across different subjects. This is still a challenging topic in the BCI community. In our case, I suspect that adding cross subject data could even improve the performance, since it seems that we are still under the curse of dimensionality.&lt;/li&gt;
&lt;li&gt;Generalizability to other experiments: It would be interesting to test if this pipeline can adapt to different types of experiments. For instance, in scenarios where discriminative features are not specific amplitude peaks (like N100 or P300) but other characteristics—such as power increases within a specific frequency band over longer periods of time, as seen in Motor Imagery experiments. This is actually one of the main selling points of foundation models: decent zero-shot performance across tasks, so it would make sense to try this one out.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I plan to explore this further in the future, but I’ll leave it here for now.&lt;/p&gt;
&lt;h1 id=&#34;tldr&#34;&gt;TLDR;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;This year there has been a surge in general-purpose foundation models for time series, along side the emergence of domain-specific models from companies like [Piramidal](&lt;a href=&#34;https://piramidal.ai/-&#34;&gt;https://piramidal.ai/-&lt;/a&gt; a YC-backed company, raising $6.5M for the foundational model &lt;em&gt;specifically&lt;/em&gt; for Electroencephalographic (EEG)&lt;/li&gt;
&lt;li&gt;This post is a personal exploration exercise to see how these general-purpose models perform with data for which domain-specific models are being built. &lt;em&gt;It is not intended as a throughout benchmark.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;For this, I chose &lt;a href=&#34;https://github.com/moment-timeseries-foundation-model/moment-research&#34;&gt;&lt;strong&gt;MOMENT&lt;/strong&gt;&lt;/a&gt;, one of the general-purpose foundation models released this year, and tested its zero-shot performance in a common brain-computer interface classification task.&lt;/li&gt;
&lt;li&gt;The results are mixed: MOMENT did not achieved the domain-specific baseline. But to be fair, the baseline is already very high.&lt;/li&gt;
&lt;li&gt;This is probably due to the limitations on how multi-channel data is processed in MOMENT. If we factor out this, the classification performance increases from around 0.6 to 0.8. Not bad 🙂&lt;/li&gt;
&lt;li&gt;The code can be found &lt;a href=&#34;https://github.com/jscastanoc/castano-blog-notebooks/blob/main/fmodel-ts-classification/moment-bci.ipynb&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;My takeaway is that the model is very promising, although the way multi-channel data is dealt with should be honed.&lt;/li&gt;
&lt;li&gt;I’m still itching to find out how MOMENT would handle other type of BCI experiments that tap into different brain responses—like Motor Imagery instead of Evoked Potentials. But I will leave this one for another post 👋🏽&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;sources&#34;&gt;Sources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2405.02358&#34;&gt;A Survey of Time Series Foundation Models: Generalizing Time Series Representation with Large Language Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.ibm.com/tutorials/awb-foundation-model-time-series-forecasting/&#34;&gt;Using foundation models for time series forecasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://neurips-time-series-workshop.github.io/&#34;&gt;NeurIPS workshop: Time Series in the Age of Large Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.14735&#34;&gt;Foundation Models for Time Series Analysis: A Tutorial and Survey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/exploring-the-latest-advances-in-foundation-time-series-models-3fc8431ab7bd&#34;&gt;Exploring the Latest Advances in Foundation Time-Series Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/datascience/comments/1e865bt/the_rise_of_foundation_timeseries_forecasting/&#34;&gt;The Rise of Foundation Time-Series Forecasting Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datadoghq.com/blog/datadog-time-series-foundation-model/&#34;&gt;Introducing Toto: A state-of-the-art time series foundation model by Datadog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.salesforceairesearch.com/moirai/&#34;&gt;https://blog.salesforceairesearch.com/moirai/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.13278&#34;&gt;Deep Time Series Models: A Comprehensive Survey and Benchmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.adesso.de/en/news/blog/foundation-models-for-time-series-applications-an-overview-of-the-leading-technologies.jsp&#34;&gt;https://www.adesso.de/en/news/blog/foundation-models-for-time-series-applications-an-overview-of-the-leading-technologies.jsp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Safe execution of AI-generated code</title>
      <link>https://jscastanoc.github.io/blog/safe-execution-of-ai-generated-code/</link>
      <pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>https://jscastanoc.github.io/blog/safe-execution-of-ai-generated-code/</guid>
      <description>&lt;p&gt;For this one I wanted to explore two options for secure execution of AI-generated code: &lt;a href=&#34;https://e2b.dev&#34;&gt;E2B&lt;/a&gt;, a cloud-based platform using microVMs, and &lt;a href=&#34;https://github.com/Jonathan-Adly/AgentRun&#34;&gt;AgentRun&lt;/a&gt;, which combines Docker-based execution with safety mechanisms. We will also examine a case study to highlight key differences between the two in terms of setup complexity and execution speed.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s get started (unless you want to jump to the conclusions &lt;a href=&#34;./blog/safe-execution-of-ai-generated-code/#tldr&#34;&gt;TLDR;&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;runtimes-for-executing-ai-generated-code&#34;&gt;Runtimes for executing AI-generated code&lt;/h2&gt;
&lt;p&gt;Dedicated runtimes for executing AI-generated code allow agentic systems to execute code without compromising the security of the host. Typical features include memory and CPU constraints, file system isolation, and restricted network access.&lt;/p&gt;</description>
      <content>&lt;p&gt;For this one I wanted to explore two options for secure execution of AI-generated code: &lt;a href=&#34;https://e2b.dev&#34;&gt;E2B&lt;/a&gt;, a cloud-based platform using microVMs, and &lt;a href=&#34;https://github.com/Jonathan-Adly/AgentRun&#34;&gt;AgentRun&lt;/a&gt;, which combines Docker-based execution with safety mechanisms. We will also examine a case study to highlight key differences between the two in terms of setup complexity and execution speed.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s get started (unless you want to jump to the conclusions &lt;a href=&#34;./blog/safe-execution-of-ai-generated-code/#tldr&#34;&gt;TLDR;&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;runtimes-for-executing-ai-generated-code&#34;&gt;Runtimes for executing AI-generated code&lt;/h2&gt;
&lt;p&gt;Dedicated runtimes for executing AI-generated code allow agentic systems to execute code without compromising the security of the host. Typical features include memory and CPU constraints, file system isolation, and restricted network access.&lt;/p&gt;
&lt;h3 id=&#34;why-not-just-a-docker-container&#34;&gt;Why not just a Docker container?&lt;/h3&gt;
&lt;p&gt;Docker containers are a common choice for isolating processes, but they have limitations in the context of executing potentially unsafe code. While Docker does provide isolation, it relies on the underlying host OS kernel, making it vulnerable to kernel-level exploits. Properly configuring Docker to execute code securely can also add some setup complexity. Finally, Docker’s performance can be hindered by overhead associated with container management, making lightweight alternatives more appealing for code execution environments.&lt;/p&gt;
&lt;h2 id=&#34;the-young-promise-in-the-space-e2b&#34;&gt;The young promise in the space: E2B&lt;/h2&gt;
&lt;p&gt;E2B recently raised a &lt;a href=&#34;https://www.crunchbase.com/funding_round/e2b-1c91-seed--6b20b90e&#34;&gt;$11.5M seed round&lt;/a&gt; for its open source platform. Unlike traditional container-based approaches, E2B uses &lt;a href=&#34;https://firecracker-microvm.github.io/&#34;&gt;Firecracker&lt;/a&gt; microVMs, which offer a lightweight and secure execution environment. Their python and javascript SDK is also packed with built-in features for output post-processing and communication with the cloud-instance.&lt;/p&gt;
&lt;h3 id=&#34;when-to-use-e2b&#34;&gt;When to Use E2B&lt;/h3&gt;
&lt;p&gt;E2B is ideal for scenarios where security, speed, and scalability are important. The platform’s use of microVMs ensures strong isolation and fast startup times.&lt;/p&gt;
&lt;p&gt;In theory, E2B is completely open source, as &lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1chsx7z/comment/l24qx1t/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button&#34;&gt;mentioned by their CEO&lt;/a&gt;. But setting it up locally is far from trivial and due to limitations on the architecture of the microVMs, currently it can only be deployed in linux machines 🥲.&lt;/p&gt;
&lt;h2 id=&#34;alternative-agentrun&#34;&gt;Alternative: AgentRun&lt;/h2&gt;
&lt;p&gt;For those who prefer not to rely on a cloud-based solution, AgentRun offers a viable local execution alternative. Unlike E2B, AgentRun uses Docker containers with extra safety mechanisms, like using &lt;a href=&#34;https://restrictedpython.readthedocs.io&#34;&gt;RestrictedPython&lt;/a&gt; and &lt;a href=&#34;https://github.com/Jonathan-Adly/AgentRun/blob/29bf2f6f17186e27d5ab2ebd80bddffc7af8a40d/agentrun/__init__.py#L159&#34;&gt;hardcoded constraints&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;when-to-use-agentrun&#34;&gt;When to Use AgentRun&lt;/h3&gt;
&lt;p&gt;While AgentRun may not offer the same level of advanced features or security guarantees as E2B’s microVM-based approach, it is well-suited for lightweight local development. It is an appealing choice for quickly testing and iterating on code without relying on cloud infrastructure.&lt;/p&gt;
&lt;p&gt;The setup is very simple: build the docker containers and install the python package. We will see the details in the next section. &lt;em&gt;Manos a la obra.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;case-study&#34;&gt;Case Study&lt;/h2&gt;
&lt;p&gt;To compare both frameworks, we will use a dataset with the columns: &lt;code&gt;[id, name, age, city, salary]&lt;/code&gt; and some dummy entries. We will query the LLM with the question: &lt;em&gt;What is the city with the highest average salary in the provided dataset and what is such salary?&lt;/em&gt;. We will use &lt;a href=&#34;https://ollama.com&#34;&gt;ollama&lt;/a&gt; to query &lt;a href=&#34;https://ollama.com/library/llama3.2&#34;&gt;llama3.2&lt;/a&gt;, one of the latest &lt;code&gt;ollama&lt;/code&gt; models supporting tool use.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Disclaimer&lt;/em&gt;: Some code snippets are taken from sample notebooks of &lt;code&gt;E2B&lt;/code&gt;, &lt;code&gt;AgentRun&lt;/code&gt;, and &lt;code&gt;ollama&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-1-install-dependencies-and-import-packages&#34;&gt;Step 1: Install dependencies and import packages&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;uv pip install e2b&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;code&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;interpreter python&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;dotenv ollama docker agentrun
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; time
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; base64
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; docker
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; os
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; dotenv &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; load_dotenv
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; e2b_code_interpreter &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Sandbox
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; agentrun &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; AgentRun
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; ollama
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;load_dotenv()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;assert&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;getenv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;E2B_API_KEY&amp;#34;&lt;/span&gt;), &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;E2B_API_KEY is not set in the .env file&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-2-build-and-run-container-agentrun-only&#34;&gt;Step 2: Build and run container (AgentRun only)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; contextlib
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; io
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; io&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;StringIO()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; contextlib&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;redirect_stdout(f): &lt;span style=&#34;color:#75715e&#34;&gt;# supress stdout&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt; docker&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;compose &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;f agentrun_docker&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;docker&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;compose&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;yml up &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;--&lt;/span&gt;build 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    load_dotenv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;./agentrun_docker/.env.dev&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    CONTAINER_NAME &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;getenv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;CONTAINER_NAME&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-3-instantiate-runtimes&#34;&gt;Step 3: Instantiate runtimes&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sbx_e2b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Sandbox()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sbx_agentrun &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AgentRun(container_name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;CONTAINER_NAME)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-4-copy-dataset-to-runtimes&#34;&gt;Step 4: Copy dataset to runtimes&lt;/h3&gt;
&lt;p&gt;E2B provides a built-in function to do so. For AgentRun, we have to do it manually by copying the dataset to our running container&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;dataset.csv&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rb&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; f:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# For E2B, we can use the built-in function&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    sbx_e2b&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;files&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;write(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/code/dataset.csv&amp;#34;&lt;/span&gt;, f)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# For AgentRun, we have to do it manually by copying &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# the dataset to our running container&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    client &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; docker&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_env()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    container &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; client&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;containers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(CONTAINER_NAME)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    container&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;put_archive(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/code/&amp;#34;&lt;/span&gt;, f&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read())
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-5-set-the-users-prompt&#34;&gt;Step 5: Set the user&amp;rsquo;s prompt&lt;/h3&gt;
&lt;p&gt;We will ask a question about the data. The first part of the prompt could also be part of the system prompt, but we will put it here for simplicity.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;messages &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;user_message &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;You are a data scientist and expert Python programmer.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;You will be asked questions about a dataset and will use Python code to analyze the data to answer these questions.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;You have access to a Python environment and can use the run_python_code tool to execute code in this environment.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;The dataset you will work with is provided as a file named &amp;#34;/code/dataset.csv.&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Use only pandas.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Question:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;What is the city with the highest average salary in the provided dataset and what is such salary?
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;messages&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append({
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;role&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;user&amp;#39;&lt;/span&gt;, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;: user_message
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;})
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-6-query-the-model&#34;&gt;Step 6: Query the model&lt;/h3&gt;
&lt;p&gt;We will use &lt;code&gt;llama3.2&lt;/code&gt;, a 8B parameters model with pretty decent scores in tool use given its size. I can run it on my laptop 🙃.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;MODEL_NAME &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;llama3.2&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;response &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ollama&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;chat(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    model&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;MODEL_NAME,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    messages&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;messages,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    tools&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;type&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;function&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;function&amp;#39;&lt;/span&gt;: {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;run_python_code&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;description&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Run python code and scripts to answer data science questions&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;parameters&amp;#39;&lt;/span&gt;: {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;type&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;object&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;properties&amp;#39;&lt;/span&gt;: {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;code&amp;#39;&lt;/span&gt;: {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;type&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;string&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;description&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;The python code to be executed&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;required&amp;#39;&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;code&amp;#39;&lt;/span&gt;],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  ],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  options&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;temperature&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;messages&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(response[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;message&amp;#39;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-7-define-the-execution-functions&#34;&gt;Step 7: Define the execution functions&lt;/h3&gt;
&lt;p&gt;We will define a single function &lt;code&gt;run_ai_generated_code&lt;/code&gt; that supports executing the E2B or the AgentRun runtime. Additionally, we will define &lt;code&gt;process_output_e2b&lt;/code&gt; and &lt;code&gt;process_output_agentrun&lt;/code&gt; since both frameworks handle output data differently.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;process_output_e2b&lt;/span&gt;(execution_output):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; execution_output&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;error:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; execution_output&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;error
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    results_idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; result &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; execution_output&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;results:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; result&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;png:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;result-&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;results_idx&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;.png&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;wb&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; f:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                f&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;write(base64&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;b64decode(result&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;png))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Saved result-&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;results_idx&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;.png&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Result &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;results_idx&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;:&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(result)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        results_idx &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt;  execution_output&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;logs&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stdout[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;process_output_agentrun&lt;/span&gt;(execution_output):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# AgentRun does not return any fancy output, just the stdout&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; execution_output
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;run_ai_generated_code&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                        ai_generated_code: str,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                        sbx_runtime: Sandbox &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; AgentRun,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                        ):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; isinstance(sbx_runtime, Sandbox):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        runner_function &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sbx_runtime&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;run_code
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; isinstance(sbx_runtime, AgentRun):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        runner_function &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sbx_runtime&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;execute_code_in_container
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ValueError&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Invalid runtime: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;sbx_runtime&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    execution &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; runner_function(ai_generated_code)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    process_output_function &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; process_output_e2b &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; isinstance(sbx_runtime, Sandbox) &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; process_output_agentrun
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; process_output_function(execution)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-8-process-the-llms-response&#34;&gt;Step 8: Process the LLM&amp;rsquo;s response&lt;/h3&gt;
&lt;p&gt;Execute code in the E2B and AgentRun runtimes, according to the tool usage defined by the model output&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;messages_e2b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; messages&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;messages_agentrun &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; messages&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# make sure the LLM decided to use the tools correctly&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; response[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;message&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;tool_calls&amp;#39;&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    available_functions &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;run_python_code&amp;#39;&lt;/span&gt;: run_ai_generated_code,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#75715e&#34;&gt;# loop through each tool use&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; tool &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; response[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;message&amp;#39;&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;tool_calls&amp;#39;&lt;/span&gt;]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        arguments &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tool[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;function&amp;#39;&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;arguments&amp;#39;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        code &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; arguments[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;code&amp;#39;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Generated code:&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(code)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		&lt;span style=&#34;color:#75715e&#34;&gt;# Execute generated code on e2b and agentrun&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; runtime_name &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;e2b&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;agentrun&amp;#34;&lt;/span&gt;]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            start_time &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; time&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;time()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;=&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Executing code in the &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;runtime_name&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; sandbox....&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            runtime &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sbx_e2b &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; runtime_name &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;e2b&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; sbx_agentrun
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            function_to_call &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; available_functions[tool[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;function&amp;#39;&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            function_response &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; function_to_call(code, runtime)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Code execution finished!&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Response from the function:&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(function_response)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            end_time &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; time&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;time()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Elapsed time: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;end_time &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; start_time&lt;span style=&#34;color:#e6db74&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;.2f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; seconds&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; runtime_name &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;e2b&amp;#34;&lt;/span&gt;:       
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                messages_e2b&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append({
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;role&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;tool&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;: function_response
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                })
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; runtime_name &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;agentrun&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                messages_agentrun&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append({
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;role&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;tool&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;: function_response
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                })
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ValueError&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Invalid runtime name: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;runtime_name&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;the-output&#34;&gt;The output:&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;    Generated code:
    import pandas as pd
    import numpy as np
    # Load the dataset
    df = pd.read_csv(&amp;#34;/code/dataset.csv&amp;#34;)
    # Group by city and calculate average salary
    avg_salary_by_city = df.groupby(&amp;#34;city&amp;#34;)[&amp;#39;salary&amp;#39;].mean()
    # Get the city with the highest average salary and its value
    highest_avg_salary_city = avg_salary_by_city.idxmax()
    highest_avg_salary = avg_salary_by_city.max()
    print(f&amp;#34;The city with the highest average salary is {highest_avg_salary_city} with an average salary of {highest_avg_salary}&amp;#34;)
    =====================================================================================
    Executing code in the e2b sandbox....
    Code execution finished!
    Response from the function:
    The city with the highest average salary is Seattle with an average salary of 73500.0
    
    Elapsed time: 0.34 seconds
    =====================================================================================
    Executing code in the agentrun sandbox....
    Code execution finished!
    Response from the function:
    The city with the highest average salary is Seattle with an average salary of 73500.0
    
    Elapsed time: 1.90 seconds
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I was expecting a difference in the execution times, but it is impressive that E2B manages to be x5.2 faster (on average, N=10) than using a local container on a decent laptop.&lt;/p&gt;
&lt;h2 id=&#34;tldr&#34;&gt;TLDR;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://e2b.dev/&#34;&gt;E2B&lt;/a&gt; is a solid option for blazing fast and secure code execution in the cloud. While straightforward support for local development is not there yet , it comes packed with many built-in features, and comprehensive documentation.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Jonathan-Adly/AgentRun&#34;&gt;AgentRun&lt;/a&gt; is lightweight and super easy to setup.  It lacks some advanced features found in E2B, it is an ideal choice for developers who prefer a simpler, self-hosted solution.&lt;/p&gt;
&lt;p&gt;You can find the code for a side-to-side comparison here: &lt;a href=&#34;https://github.com/jscastanoc/ai-code-runtime&#34;&gt;https://github.com/jscastanoc/ai-code-runtime&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://e2b.dev/&#34;&gt;https://e2b.dev/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Jonathan-Adly/AgentRun&#34;&gt;https://github.com/Jonathan-Adly/AgentRun&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=XmExYTHmvbw&#34;&gt;Video: E2B: The Missing Piece for AI Agents?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1chsx7z/is_there_an_opensource_alternative_to_e2b_e2bdev/&#34;&gt;Reddit: is there an open source alternative to e2b?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>About</title>
      <link>https://jscastanoc.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://jscastanoc.github.io/about/</guid>
      <description>&lt;h1 id=&#34;who-am-i&#34;&gt;Who am I?&lt;/h1&gt;
&lt;p&gt;I am Sebastian, a freelance data scientist, AI/ML expert, and tech lead. I have a PhD in machine learning and neuroscience from the University of Freiburg (Germany), and a background in electronics engineering.&lt;/p&gt;
&lt;h1 id=&#34;what-experience-do-i-bring&#34;&gt;What experience do I bring?&lt;/h1&gt;
&lt;p&gt;Over the past 10 years, I have worked across academia, corporate, and startups, bringing AI/ML solutions to life across a wide range of domains&amp;mdash;from developing cutting-edge, clinical-grade brain-computer interfaces (feel free to check out my &lt;a href=&#34;https://scholar.google.com/citations?user=Ge0wai4AAAAJ&amp;hl=en&#34;&gt;publications&lt;/a&gt;, if you are curious), to handling massive datasets for building AI/ML products in corporate environment, and giving non-technical teams the insights they need to make smart decisions at PepsiCo.&lt;/p&gt;</description>
      <content>&lt;h1 id=&#34;who-am-i&#34;&gt;Who am I?&lt;/h1&gt;
&lt;p&gt;I am Sebastian, a freelance data scientist, AI/ML expert, and tech lead. I have a PhD in machine learning and neuroscience from the University of Freiburg (Germany), and a background in electronics engineering.&lt;/p&gt;
&lt;h1 id=&#34;what-experience-do-i-bring&#34;&gt;What experience do I bring?&lt;/h1&gt;
&lt;p&gt;Over the past 10 years, I have worked across academia, corporate, and startups, bringing AI/ML solutions to life across a wide range of domains&amp;mdash;from developing cutting-edge, clinical-grade brain-computer interfaces (feel free to check out my &lt;a href=&#34;https://scholar.google.com/citations?user=Ge0wai4AAAAJ&amp;hl=en&#34;&gt;publications&lt;/a&gt;, if you are curious), to handling massive datasets for building AI/ML products in corporate environment, and giving non-technical teams the insights they need to make smart decisions at PepsiCo.&lt;/p&gt;
&lt;h1 id=&#34;who-do-i-work-with&#34;&gt;Who do I work with?&lt;/h1&gt;
&lt;p&gt;I work with companies to identify valuable (and quantifiable!) AI/ML opportunities, translating business problems into technical designs, developing proof-of-concepts, and building the teams and infrastructure needed to deploy scalable solutions into production.&lt;/p&gt;
&lt;p&gt;I also work with existing tech teams, providing support on the R&amp;amp;D-heavy side of AI/ML  and data science projects.&lt;/p&gt;
&lt;h1 id=&#34;what-am-i-really-good-at&#34;&gt;What am I really good at?&lt;/h1&gt;
&lt;p&gt;While I have significant experience across a wide range of ML techniques, from classic to deep learning models for image, audio, text, and all sorts of data (even brain data)&amp;mdash;both teaching as well as in real-world scenarios&amp;mdash;, I have particularly deep expertise in time-series analysis and using AI/ML in data-scarce environments.&lt;/p&gt;
&lt;p&gt;I also love tinkering around with hardware prototypes.&lt;/p&gt;
&lt;h1 id=&#34;about-my-working-style&#34;&gt;About my working style&lt;/h1&gt;
&lt;p&gt;People I work with can expect a problem solver and a builder, with an impact/value-driven approach. I bring deep technical knowledge, scientific rigor, and a strong desire to understand business/domain specific needs and constraints. I enjoy working in fast-paced, interdisciplinary environments where learning fast and cross-functional teamwork are key. I also like mentoring more junior colleagues&amp;mdash;I think I am a good communicator and love to help others grow. Finally, if you’re into it, I’m always up for geeking out about drums, mountains, or psychology.&lt;/p&gt;
&lt;p&gt;Feel free to reach out, either about specific projects or just to chat. I am always more than happy to connect.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Publications</title>
      <link>https://jscastanoc.github.io/pubs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://jscastanoc.github.io/pubs/</guid>
      <description>&lt;p&gt;Here are some highlights. You can find the full list &lt;a href=&#34;https://scholar.google.com/citations?view_op=list_works&amp;hl=en&amp;hl=en&amp;user=Ge0wai4AAAAJ&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Castaño-Candamil (2020), Machine learning methods for motor performance decoding in adaptive deep brain stimulation.  Doctoral dissertation. Universität Freiburg. &lt;a href=&#34;https://freidok.uni-freiburg.de/data/175124&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Castaño-Candamil et al. (2020). A pilot study on data-driven adaptive deep brain stimulation in chronically implanted essential tremor patients. &lt;em&gt;Frontiers in Human Neuroscience, 14&lt;/em&gt;, 541625. &lt;a href=&#34;https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2020.541625/full&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Castaño-Candamil et al. (2015). Solving the EEG inverse problem based on space–time–frequency structured sparsity constraints. &lt;em&gt;NeuroImage&lt;/em&gt;, 118, 598-612. &lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S1053811915004346&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;</description>
      <content>&lt;p&gt;Here are some highlights. You can find the full list &lt;a href=&#34;https://scholar.google.com/citations?view_op=list_works&amp;hl=en&amp;hl=en&amp;user=Ge0wai4AAAAJ&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Castaño-Candamil (2020), Machine learning methods for motor performance decoding in adaptive deep brain stimulation.  Doctoral dissertation. Universität Freiburg. &lt;a href=&#34;https://freidok.uni-freiburg.de/data/175124&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Castaño-Candamil et al. (2020). A pilot study on data-driven adaptive deep brain stimulation in chronically implanted essential tremor patients. &lt;em&gt;Frontiers in Human Neuroscience, 14&lt;/em&gt;, 541625. &lt;a href=&#34;https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2020.541625/full&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Castaño-Candamil et al. (2015). Solving the EEG inverse problem based on space–time–frequency structured sparsity constraints. &lt;em&gt;NeuroImage&lt;/em&gt;, 118, 598-612. &lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S1053811915004346&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Castaño-Candamil et al. (2020). Identifying controllable cortical neural markers with machine learning for adaptive deep brain stimulation in Parkinson’s disease. &lt;em&gt;NeuroImage: Clinical, 28&lt;/em&gt;, 102376. &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2213158220302138&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Castaño-Candamil et al. (2019). Post-hoc labeling of arbitrary M/EEG recordings for data-efficient evaluation of neural decoding methods. &lt;em&gt;Frontiers in Neuroinformatics, 13&lt;/em&gt;, 55. &lt;a href=&#34;https://www.frontiersin.org/journals/neuroinformatics/articles/10.3389/fninf.2019.00055/full&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
